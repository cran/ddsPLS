<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Introduction</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>

<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<p>In the following \(\mathbf{X}\in\mathbb{R}^{n\times p}\) is a matrix of \(n\) rows and \(p\) columns describing the covariate part of a data set in a mono-block context. If this is a multi-block analysis, with one block for <strong>Proteomics</strong>, one block for <strong>DNA methylation</strong> for example and other \(K-2\) blocks, then the covariate data set is built on \(K\) blocks and is denoted as 
\[
\mathbf{X}_s=\{\mathbf{X}_{proteo},\mathbf{X}_{DNA},\cdots\}\in \mathbb{R}^{n\times p_{proteo}}\times\mathbb{R}^{n\times p_{DNA}}\times\cdots
\]
\(\mathbf{X}_s\) is treated with <em>list()</em> structure by the package, so in case of multi-block analysis, thanks to present your dataset such as</p>

<pre><code class="r"># For no named matrices
Xs &lt;- list(X1,X2,X3)

# For names matrices
Xs &lt;- list(X_dna=X1,X_proteo=X2,X_RNA=X3)
</code></pre>

<p>if the covariate part is filled with only one data set, it is possible to use the <em>list()</em> structure or not. </p>

<p>In the other cases, the number of variables in each matrix can be different. In the previous examples we have denoted by \(p_{proteo}\) and \(p_{DNA}\) two of the different number of variables. It is knowd that <strong>Proteomics</strong> data-sets are filled with hundreds of variables while <strong>DNA methylation</strong> data sets gather hundreds of thousands of variables.</p>

<p>The supervizion part is described by the \(\mathbf{Y}\in\mathbb{R}^{n\times q}\) matrix. In case of regression it can be a <em>vector</em> for univariate case (\(q=1\)) or a <em>matrix</em> or even a <em>data.frame</em>. In multi-variate regression, \(q>1\), it can be a <em>matrix</em> or a <em>data.frame</em>. In case on classification, \(\mathbf{Y}\) must be unidimensionnal but it can be a <em>vector</em> or a <em>matrix</em> or even a <em>data.frame</em>. The argument <em>mode</em> permits to choose between regression mode, then use <em>reg</em>, and classification mode, then use something else.</p>

<p>In the \(\mathbf{X}_s\) and \(\mathbf{Y}\) data sets, the same numbers of individuals are described, \(n\) and each of them are on the same row</p>

<p><i class="fa fa-exclamation-triangle fa-3x" aria-hidden="true"></i> <strong>No individual can be missing in all the \(\mathbf{X}\) matrices and no missing entry is allowed in the \(\mathbf{Y}\) dataset</strong>.</p>

<h1>Introduction</h1>

<p>The present package [based on @lorenzo2019supervised]. It permits to take into account missing values in a multi-block context for supervized problems. It also permits to deal with data analyses with no missing values. Regression or classification regression paradigms are accepted.</p>

<p>Only two parameters must to be fixed by the user :</p>

<ul>
<li><p><strong>R</strong> which is the number of components, analogous to the number of components in PCA.</p></li>
<li><p><strong>\(L_0\in\mathbb{N}^\star\)</strong> (or <strong>\(\lambda\in[0,1]\)</strong>) which is the regularization coefficient. \(L_0\) can be directly interpreted as the maximum number of variables to be selected in the model.and \(\lambda\) is the minimum of correlation value under which an interaction between a variable of a block \(\mathbf{X}\) and one of the \(\mathbf{Y}\) variables is not taken into account.</p></li>
</ul>

<p>Those two parameters are to be fixed with cross validation. We have developped a parallelizable code, using <strong>doParallel</strong> package [see @doparalelManual]. This is transparent for the user, which hase just to fix the number of <em>cpus</em> to be used. The corresponding parameter is denoted as <strong>NCORES</strong>. If it equals \(1\), then no parallelization structure is deployed.</p>

<p>To load the <strong>ddsPLS</strong> package:</p>

<pre><code class="r">library(ddsPLS)
</code></pre>

<p>In the following, only <strong>regression</strong> examples are shown. To perform classification analyses, please change the argument</p>

<pre><code class="r">mode
</code></pre>

<p>to <strong>lda</strong> of <strong>logit</strong> if you want to perform classification thanks to linear discriminant analysis or logistic regression (only 2 classes allowed for <strong>logit</strong>).</p>

<h2>Classification case</h2>

<pre><code class="r">data(&quot;penicilliumYES&quot;)
X &lt;- penicilliumYES$X
X &lt;- scale(X[,which(apply(X,2,sd)&gt;0)])
Xs &lt;- list(X[,1:1000],X[,-(1:1000)])
Xs[[1]][1:5,]=Xs[[2]][6:10,] &lt;- NA
Y &lt;- as.factor(unlist(lapply(c(&quot;Melanoconidiu&quot;,&quot;Polonicum&quot;,&quot;Venetum&quot;),function(tt){rep(tt,12)})))

mddsPLS_model_class &lt;- mddsPLS(Xs = Xs,Y = Y,L0=3,R = 2,mode = &quot;lda&quot;,verbose = TRUE)
</code></pre>

<p>is considered a mult-block classification case analysis with missing samples. <strong>mode=&ldquo;lda&rdquo;</strong> is used since there are 3 classes to discriminate.</p>

<h1>Mono-block</h1>

<p>The here used method can be used on mono and multi-block datasets with no missing values.
In the case on mono-block datasets, there is no current developpment to deal with missing samples. Actually this would imply that for a given individual, no covariate is known. Which block any inference.</p>

<h2>Regression case {.tabset}</h2>

<p>The regression case has been treated through a toy example well know dataset in that section.</p>

<h3>Build a model {.tabset}</h3>

<p>We have worked on the Liver Toxicity dataset [see @bushel2007simultaneous]. This data set contains the expression measure of 3116 genes and 10 clinical measurements for 64 subjects (rats) that were exposed to non-toxic, moderately toxic or severely toxic doses of acetaminophen in a controlled experiment. Therefore the structure is :
\[\mathbf{X}\in\mathbb{R}^{64\times3116},\mathbf{Y}\in\mathbb{R}^{64\times10}\]</p>

<p>The following commands permit to create the model. The parameter <em>verbose</em> shows main information about the model.</p>

<pre><code class="r">data(&quot;liverToxicity&quot;)
X &lt;- scale(liverToxicity$gene)
Y &lt;- scale(liverToxicity$clinic)
mddsPLS_model_reg &lt;- mddsPLS(Xs = X,Y = Y,L0=10,R = 3,
                             mode = &quot;reg&quot;,verbose = TRUE)
</code></pre>

<pre><code>## At most 10 variable(s) can be selected in the X part
##     For each block of X, are selected in order of component:
##         @ (9,1,9) variable(s)
##     For the Y block, are selected in order of component:
##         @ (2,2,1) variable(s)
</code></pre>

<h4>Plot Method {.tabset}</h4>

<p>The <strong>Plot</strong> mehtod permits many types of representation. Among which</p>

<ul>
<li><em>weights</em> which shows all the weights under barplot representations. If <strong>super</strong> is set to <strong>TRUE</strong>, then the <strong>super-weights</strong> are represented. In the mono-block case, <strong>super-weights</strong> are equal to <strong>weights</strong>.</li>
<li><em>heatmap</em> which shows the selected variables (for \(\mathbf{X}\) and \(\mathbf{Y}\)) for the all individuals.</li>
</ul>

<p>The following sections detailed the discussed method though those parametizations.</p>

<h5><em>Super-Weight</em> representation</h5>

<p>The following figure permits to represent the <em>super-weights</em> for the first and the second components for block 1 (mono-block case actually) and block Y. <strong>mar_left</strong> parameter adds extra values on the left of the plot, usefull to see variable names.</p>

<pre><code class="r">plot(mddsPLS_model_reg,vizu = &quot;weights&quot;,variance = &quot;Linear&quot;,
     super = T,comp = c(1,2),addY = T,mar_left = 3,mar_bottom = 3,
     pos_legend = &quot;topright&quot;,reorder_Y = T,legend.cex = 1.5)
</code></pre>

<p><img src="data:image/png;base64,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" title="plot of chunk unnamed-chunk-7" alt="plot of chunk unnamed-chunk-7" width="1000px" height="1000px" /></p>

<h5><em>Heatmap</em> representation</h5>

<p>Heatmap permit to appreciate the discriminative aspect of variables according to their individual representations. It has been decided to plot heatmaps component per component, only the first one here</p>

<pre><code class="r">plot(mddsPLS_model_reg,vizu = &quot;heatmap&quot;,comp = 1)
</code></pre>

<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAADSCAMAAADHTA2aAAABNVBMVEUAAAAAADoAAGYAOjoAOmYAOpAAZmYAZpAAZrY6AGY6Ojo6OmY6OpA6ZmY6ZrY6kJA6kLY6kNtmAABmADpmAGZmOgBmOmZmOpBmZgBmZjpmZmZmZpBmZrZmkGZmkJBmkLZmkNtmtmZmtrZmtttmtv99ACWQOgCQOjqQOmaQZgCQZjqQZmaQZpCQkDqQkGaQkJCQkLaQkNuQtpCQttuQ27aQ29uQ2/+2ZgC2Zjq2Zma2kDq2kGa2kJC2tpC2tra2ttu2tv+225C227a229u22/+2/7a2/9u2///DIgDbkDrbkGbbkJDbtmbbtrbb25Db29vb2//b/7bb/9vb///hPADtYgDyhAD1oQD3ujz40HT75Jr/AAD/tmb/tpD/trb/25D/27b/29v/9Lf//7b//8j//9v///+9d8bjAAAACXBIWXMAAASdAAAEnQF8NGuhAAALLElEQVR4nO2dC1vbthrHBSODXUqTtpSu3YVDMhe60bVbk3BKSHrOdkrDBlsJ5ELZQoH4+3+Eo5sV+Rq/juMo5P3zkMjSKyf6PbL5I8k2sVGxRbL6oGbper0lU96S2Luw7et79KW/0cxv7Wyu//D81deR0bF3HU/ZwSIkt33ny+3Nh2u57bv3WGp9e/lphTZ/cefT+5XqQsVu3n3Ak7yYljXpRpO+V+98+Xr5adnKtUT7dytVsrJb3tpZfF6hu6apR3RnCyzNQu+sVlllRnU11TZk2bMerpGFr8jiTom27u3WDln8qnJRoCXFrYOLwm6FQRjwJCumZXm20Sw1ixatV6KJku3Aoj+fl7cOrp/8+MOBfXFntcl2RksHNPTVfVrMKs40rPXXtHNRWLlnjMYjsviIsN5AKdB3Bos2eYclRfFCheUzRk2Se83enJ51WLmgOWTFPiwzWNe0MzJY/MCjH7H54JGs2N9ItQ2ZwQoSA5SkTGdAonQYsZMEmiqsNBTVgLQbh7BS+qyZEMICCGEBhLAAQlgAISyAEBZACAsghAUQwgIIYYUr8v9mIS047Q9PeX+T1ujvi7CUEBZACAsghAUQwgIoNiz/38YsPnyKiuETvGZB5TjMUv0+ae4sbcX8cv4DD2HFCXPDSvtQvMWwFKbbBSvuuSmkp4TBcjazgiW/WJWUamJVB387aqX18bG+xKhww2DVWrWfjj876H92UPujZPNp9DQ1UVgpnrfi7aRKFr958v7Jjyd/VVfsFHoW7DAb9Z1HwJKvKSg2rG+3SYEdjbRnpQBrrO8UCsuxWOzVBSulrjWdE3z4p0b1ruAuqIXLLiT7lQ4rnYYaBysiwpdH3C/ThvXhA3u9/tr+O9XFOynDUp3MdsNSx2NKB2IsWPbFZvjazbgfFOuE7jqiwqq6Yx06dgCsYSKVvhVzD/8+mMgH+U9Afm/kO4VpnU/RcU7s4nfKsGpjOyvfnzBvT9FguTuQq7ZWR8UGgpkirPE+I+hPHHGngo+pYbGnmqIURGi2YfkTnr/8nm7iHFPDSkSrZusn7lsFSz9s9IR6c/UnT3OdSgrfqJPSbMNymqY12JXndJIQWETrQ3rylsIa3TSHRFAxAdKYE1iZFI/fmokKYc2tJgPrA5dt93rdbo+98DehrqMOVdenjlCb6txRr6fVVm+shIW1RaUrJpnX7V5dsVy5v7Mzvs+xm4WwAEJYACEsgFKFpey6qbCaxcbAWrGrBXt3szFtWE7CUFiDxt5a0cod1xt9K2+VLhFWjMMw6sLPWO0DVwySF5YGQTVXMhJMXJw4BAnr7EzA6jEoHtA8IQopGA2tgMV3e3MjYbXbLIrvN2lTU4AVlYewRldEWICKCAtQ0WcdEFaMiobCGljLVq6V1GXNGSz740nD7id1WfMGiyupy0oIyzut5VMArEi1pbnsMggqV5lS14Ymb232otXnJVpOMkJ6u5NWQliASggLUAlhASoRO/jfwzCfZQKsgbX08Hmlurl+8vRxJYGDGAtWaIChsOzqwqZVtPIv+vX9RgIHMV+wdCVwEMlhRbkHz3iW8+Zl1HEkPJEcj1KOSlZjOey3KwaxpOUSCb4TZ0SL74gbr2737OzmRgJsSzM2pX93yKi6CMtXCWEBKiEsQKXR1gFhxahkKiw+FZZr9Rv/vE0yRjNXsPhUWMO+/LOaO04yRjNXsMbVpGGJ6b1z1XKNkhddV0E9F3wkOceCKkZqj3w8S5LodB2+HTG+xfLPmFgOGx5rJ2kqrN2JKiEsQCWEBaiEsEIqRfsso2ANrOXG31/U3yYboEkFVkCAsbBy20s/P/hPsgEaMCzPApAZg6Up0RQPFJa/UvjUtICl6dwxRwqL8lQuP+aWJ0+r7Y2Ws1+CDtPp6SlbbtRuT+WchbDA4QgLEI6wAOEhsIbWwVRY4mkh1RL7u/iscljZpWnAqT5VWB5sxsEaNGqtwX7DKtCUffnr/pvLX63C5W8IK/gwvL63ol7hmjNY42kysHRTOkG5YLkNrkR1quE6GwOTt4GAcIQFCA+Epf8XhLCG4cGwRMo36oCwImCxF1NhNYvWsrViZzS7M9uw2OxO7s9WVrM7sw1rXKUKS411ZQLLY850VGIA6/T03bt3jBob3DIN1vANYQ3DERYg3A9rONaMsEbDGvaszHxWAljyGulsrYN74ssFi8tQWMw6lD5mbB38WbMBa1whrPFaDwwPghUwnqWapm3LPDHdJ4vVDKEWqYrV9KNnj3JFknZFP/OhEhYjx6wpwkJYtxoWUfcvDJ/dQVgOLG++qbD4VFiZze5YS2sFlqqWqkWrhLD8sMRU2Bs2b0jNaZ5Nih21jk4oPIQVcBhOYSpsJKzhVWG92ILE9pxpL22TrzFifqrrLFHmOj1li5anOBXmzUJYccMRFiAcYQHCyXBE2bcA1zBY7PYqK83SoF5/+rhyWG4MgOM0KcHyyFBY9scTa+mX2rGVr1PbkG/0geM08wVrTCGs8VoPDA+C5ZhSrxONRcc7ShUQ0GXX1MukYMb9qAOx43DS5g2ncFUYwhovHGEBwiUsl3VAWCNgufIMheVerVwtifXKWcIiZFZguVcr/3Z0ItYrZwjLnqGeNfUhmihYoaDSUHcoxUiuVxaGS6JimsoF5Qhr7HDiWn1rI6yocOIdpEFY4eHOg22ipsKMgOWa3SlaefpTAJiHVGENM8yE5Z7dKVQZrgLAPMwVLEOsA3Hf1g4OizVYXTbOmy9sk188xJkeU7A6LonVyvJCJzY8w61XIkCjWw8M11d1iwyEFRqOsADhCAsQrl3jlNQ6zB0sLcNQWJrPym2z9TOFQz5Qg7D8sHSfZR+dMFPKpsT232QKy3szXENhGeGzfPmxYamWK2baiqKRfKUBFQuOeg4xPpYllhydsokwITNW/gXlI6yocIQFCEdYgHB9gAbiszKG5RqiKfHFt3yYJntY+qaZsDxDNMI8IKyww9AU6xAES1t4rE1weZcji3x1n2AvtaBwl8sStxvuiHXKQ15iRMshZ9g5S99EWBHhCAsQjrAA4cTlHhBWNCzOhuibJsJyraIpU9PAfFaWq2i0ItNhee75B3RZcwbLEJ9l2665sBBTOgQQrhghGs2u86bdqlmaUjlvaOD/hqyI+HsWwgorQljxwxEWIJwQv88yDZZmHZbW2PXRkCukU4alpY2E5bIOYg0N4ArpNGHZ5sMyxzqEwZKEpHHysVAA5cMxleFySTNhPE7zWWLKjNsrQU06rU5HwhKTYWadsxAWJBxhAcIRFiA8ZHYHYY0KMxSWaypMjDkUs5/dmQ1YnnvR8PXK/BdhBR2GxvgsTfqTMzV1nGdkau08Hz5SXA1EtcUjrTwsnRjBQlzr1XWcqbpfD5tGpLq66vWumLrdmxu5AaeTrPXAcIQFCEdYgHCE5QonUTIVFjUMz0qH5cZhZffV+l5563m5MYA8GDIxrMgwQ2HZ1YW7dStfp+aq+JNVrLPb0UAeDDlfsIIV+2lhUFjDQy1KsNsFe5YcccWoJHmyASyFUSxSPvv++6AqsKYGtX4i9RAWoB7CAtRDWO56Ma2DUbDYbewWiv8cZPSkgZgyFtbS2tLa4n8zetJATBkKa1whLIAmA+uWCmEBhLAAQlgAISyAEBZACAsghAUQwgIIYQGEsABCWAAhLIAQFkAICyBTYf1+QF+uX9Zatl1rXb8UmWxrmjIW1r/uVy4+2dj75fpN7Y9PNmrvX14sVGqvc1PFZSysx/978aK/UavVD/a+6zNY+1ahtr2KsAL0+7fvX1ZJoXa8ate2SaFK8hYp7K2R+BfaTECmwjJS/wchjl08PMD/6QAAAABJRU5ErkJggg==" title="plot of chunk unnamed-chunk-8" alt="plot of chunk unnamed-chunk-8" width="1000px" height="1000px" /></p>

<p>Each variable has been normalized separately. Dendrogram are used in the individual and in the variable directions using <strong>hclust</strong> similarities.</p>

<h4>Summary Method</h4>

<p>A <em>summary</em> method explains more precisely the model.</p>

<pre><code class="r">summary(mddsPLS_model_reg)
</code></pre>

<pre><code>## ===============================================================
##                      ddsPLS object description              
## ===============================================================
## 
## Number of blocks: 1
## Number of dimensions: 3
## Regularization coefficient: 10
## Number of individuals: 64
## Number of variables in Y part: 10
## Model built in mode regression
## Maximum number of iterations in the imputation process: 50
## Algorithm of imputation has converged
## 
## 
##           Variance Explained (%)    
## -------------------------------------------
## Total Y variance explained by all the Super Components
## [1] 66
## Total Y variance explained by each Super Component
## [1] 60 55 54
## 
## Marginal Y variable variance explained by each Super Component
##                    Super Comp. 1 Super Comp. 2 Super Comp. 3
## BUN.mg.dL.                  69.0          61.0          54.0
## Creat.mg.dL.                15.0          19.0           9.0
## TP.g.dL.                     1.7           4.6          15.0
## ALB.g.dL.                   15.0           4.6           7.6
## ALT.IU.L.                   96.0          86.0          84.0
## SDH.IU.L.                   10.0          18.0          25.0
## AST.IU.L.                   95.0          84.0          83.0
## ALP.IU.L.                   37.0          36.0          33.0
## TBA.umol.L.                 84.0          82.0          90.0
## Cholesterol.mg.dL.          66.0          57.0          46.0
## 
## Total Y variance explained by each component of each block
##         Comp. 1 Comp. 2 Comp. 3
## Block 1      60      54      55
## 
## 
##                     RV coefficient    
## -------------------------------------------------------
## Total Y with all the Super Components
## [1] 0.32
## 
## Total Y with each Super Component
## [1] 0.36 0.30 0.29
## 
## Each Y variable with each Super Component
##                    Super Comp. 1 Super Comp. 2 Super Comp. 3
## BUN.mg.dL.               0.47000        0.3800        0.2900
## Creat.mg.dL.             0.02300        0.0350        0.0082
## TP.g.dL.                 0.00028        0.0021        0.0220
## ALB.g.dL.                0.02100        0.0021        0.0058
## ALT.IU.L.                0.92000        0.7500        0.7100
## SDH.IU.L.                0.01000        0.0310        0.0600
## AST.IU.L.                0.91000        0.7000        0.6900
## ALP.IU.L.                0.14000        0.1300        0.1100
## TBA.umol.L.              0.70000        0.6700        0.8100
## Cholesterol.mg.dL.       0.43000        0.3300        0.2100
## 
## Total Y with each component of each block
##         Comp. 1 Comp. 2 Comp. 3
## Block 1    0.36    0.29     0.3
## 
## 
##          Missing value information    
## -------------------------------------------
## 
##                                     X1 Total
## Number of variables               3116  3116
## Number of missing samples            0     0
## Proportion of missing samples (%)    0     0
## 
## 
##               mddsPLS results         
## -------------------------------------------
## 
## At most 10 variable(s) can be selected in the X part
## 
## 
##  ---- For each block of X, are selected
##   Super Comp. 1 Super Comp. 2 Super Comp. 3
## 1             9             9             1
##  ---- For the Y block, are selected
##         @ (2,2,1) variable(s)
## 
## 
##                  Thank&#39;s for using me      
## ---------------------------------------------------------------
##                                                 Hadrien Lorenzo
##                                  hadrien.lorenzo.2015@gmail.com
## ===============================================================
</code></pre>

<h4>Get selected variables</h4>

<p>Output <em>var_selected</em> permits to get the selected variables per block.</p>

<p>This is a list filled with \code{2R}-column matrices corresponding to the non nul coefficients of each block on the \emph{Super-Components}. They are ordered according to their absolute value on the first <strong>Super-Component</strong>, in a decreasing way.</p>

<pre><code class="r">mddsPLS_model_reg$var_selected[[1]]
</code></pre>

<pre><code>##              Weights_comp_1 Weights_comp_2 Weights_comp_3
## A_42_P620915   -0.591212926              0    -0.45490352
## A_42_P809565    0.000000000              1     0.00000000
## A_42_P840776   -0.156508320              0    -0.61046161
## A_43_P14131    -0.737716168              0     0.37689499
## A_43_P23376    -0.069399953              0    -0.21628484
## A_42_P675890   -0.031981983              0    -0.16317508
## A_43_P10606    -0.029507817              0    -0.15055165
## A_43_P17415    -0.273185553              0     0.41434680
## A_42_P758454   -0.019127771              0    -0.09759168
## A_42_P802628   -0.006414699              0    -0.03272840
##              Super_Weights_comp_1 Super_Weights_comp_2
## A_42_P620915          0.591212926           0.45490352
## A_42_P809565          0.000000000           0.00000000
## A_42_P840776          0.156508320           0.61046161
## A_43_P14131           0.737716168          -0.37689499
## A_43_P23376           0.069399953           0.21628484
## A_42_P675890          0.031981983           0.16317508
## A_43_P10606           0.029507817           0.15055165
## A_43_P17415           0.273185553          -0.41434680
## A_42_P758454          0.019127771           0.09759168
## A_42_P802628          0.006414699           0.03272840
##              Super_Weights_comp_3
## A_42_P620915                    0
## A_42_P809565                   -1
## A_42_P840776                    0
## A_43_P14131                     0
## A_43_P23376                     0
## A_42_P675890                    0
## A_43_P10606                     0
## A_43_P17415                     0
## A_42_P758454                    0
## A_42_P802628                    0
</code></pre>

<h3>Cross Validation {.tabset}</h3>

<p>The cross-validation process is started in a leave-one-out design along \(1\) dimension. <strong>NCORES</strong> fixes the number of cores in the paralellized process, <strong>n_lambda</strong> fixes the number of regularization coefficients to be tested.</p>

<pre><code class="r">n_lambda &lt;- 50
NCORES &lt;- 7

res_cv_reg_L0 &lt;- perf_mddsPLS(Xs = X,Y = Y,
                           R = 1,L0s=1:50,
                           mode = &quot;reg&quot;,NCORES = NCORES,kfolds = &quot;loo&quot;)

res_cv_reg &lt;- perf_mddsPLS(Xs = X,Y = Y,
                           R = 1,lambda_min=0.5,n_lambda=n_lambda,
                           mode = &quot;reg&quot;,NCORES = NCORES,kfolds = &quot;loo&quot;)
</code></pre>

<h4>Error Plot {.tabset}</h4>

<p>A <em>plot</em> method plots the results. <em>plot_mean</em> permits to add the mean of the <strong>Y</strong> variable prediction errors. <em>legend_names</em> permits to add a legend with the names of the <strong>Y</strong> variables. <em>pos_legend</em> permits to choose the position of the legend. <em>which_sd_plot</em> picks the <strong>Y</strong> variables whose standard error must be drawn.</p>

<h5>Using \(L_0\) paradigm</h5>

<pre><code class="r">plot(res_cv_reg_L0,which_sd_plot = c(5,7),ylim=c(0,1.1),alpha.f = 0.4,
     plot_mean = T,legend_names = colnames(Y),pos_legend = &quot;bottomright&quot;,no_occurence = T)
</code></pre>

<p><img src="data:image/png;base64,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" title="plot of chunk unnamed-chunk-13" alt="plot of chunk unnamed-chunk-13" width="1000px" height="1000px" /></p>

<p>Minimum for \(L_0\approx 18\).</p>

<h5>Using \(\lambda\) paradigm</h5>

<pre><code class="r">plot(res_cv_reg,which_sd_plot = c(5,7),ylim=c(0,1.1),
     alpha.f = 0.4,plot_mean = T,legend_names = colnames(Y),
     no_occurence = T)
</code></pre>

<p><img src="data:image/png;base64,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" title="plot of chunk unnamed-chunk-14" alt="plot of chunk unnamed-chunk-14" width="1000px" height="1000px" /></p>

<p>Minimum for \(\lambda\approx0.85\).</p>

<h5>Observations</h5>

<p>Are also plotted vertical lines representing:</p>

<ul>
<li>The global minimum,</li>
<li>The minimum of the <strong>Mean MSEP</strong>.</li>
</ul>

<p>The two panels building this plot are headed with numbers. Red for the error curves and blue for the \(\mathbf{Y}\) variable occurence curves. It represents the number of variables selected in the \(\mathbf{X}\) and in the \(\mathbf{Y}\) respectively for the model built on the matrices given to the function.</p>

<p>If one would not prefer to see the occurence plot. The parameter <em>no_occurence</em>, when set to <strong>TRUE</strong>, permits to hide it.</p>

<h4>Summary Method</h4>

<p>A <em>summary</em> methods explains more precisely the model. The parameter <em>plot_res_cv</em> uses the <strong>plot</strong> method. Are also presented the statistical results of the algorithm (convergence for each fold, time of computation).</p>

<pre><code class="r">summary(res_cv_reg,plot_res_cv =F)
</code></pre>

<pre><code>## ======================================================
##       Cross-Validation ddsPLS object description      
## ======================================================
## 
## Number of blocks: 1
## Number of individuals: 64
## Number of variables in Y part: 0
## Model built in mode regression
## 
## 
##     Missing value information    
## ---------------------------------
## 
##                                     X1 Total
## Number of variables               3116  3116
## Number of missing samples            0     0
## Proportion of missing samples (%)    0     0
## 
## 
##      Cross-Validation results    
## ---------------------------------
## 
##    R    lambda Nb.of.convergences.VS.nb.of.fold
## 1  1 0.5000000                            64/64
## 2  1 0.5086403                            64/64
## 3  1 0.5172806                            64/64
## 4  1 0.5259209                            64/64
## 5  1 0.5345611                            64/64
## 6  1 0.5432014                            64/64
## 7  1 0.5518417                            64/64
## 8  1 0.5604820                            64/64
## 9  1 0.5691223                            64/64
## 10 1 0.5777626                            64/64
## 11 1 0.5864029                            64/64
## 12 1 0.5950431                            64/64
## 13 1 0.6036834                            64/64
## 14 1 0.6123237                            64/64
## 15 1 0.6209640                            64/64
## 16 1 0.6296043                            64/64
## 17 1 0.6382446                            64/64
## 18 1 0.6468849                            64/64
## 19 1 0.6555251                            64/64
## 20 1 0.6641654                            64/64
## 21 1 0.6728057                            64/64
## 22 1 0.6814460                            64/64
## 23 1 0.6900863                            64/64
## 24 1 0.6987266                            64/64
## 25 1 0.7073669                            64/64
## 26 1 0.7160071                            64/64
## 27 1 0.7246474                            64/64
## 28 1 0.7332877                            64/64
## 29 1 0.7419280                            64/64
## 30 1 0.7505683                            64/64
## 31 1 0.7592086                            64/64
## 32 1 0.7678489                            64/64
## 33 1 0.7764891                            64/64
## 34 1 0.7851294                            64/64
## 35 1 0.7937697                            64/64
## 36 1 0.8024100                            64/64
## 37 1 0.8110503                            64/64
## 38 1 0.8196906                            64/64
## 39 1 0.8283309                            64/64
## 40 1 0.8369711                            64/64
## 41 1 0.8456114                            64/64
## 42 1 0.8542517                            64/64
## 43 1 0.8628920                            64/64
## 44 1 0.8715323                            64/64
## 45 1 0.8801726                            64/64
## 46 1 0.8888129                            64/64
## 47 1 0.8974531                            64/64
## 48 1 0.9060934                            64/64
## 49 1 0.9147337                            64/64
## 50 1 0.9233740                            64/64
##    Mean.AND.sd.time.of.computation
## 1                      0.29(0.025)
## 2                      0.29(0.029)
## 3                      0.29(0.029)
## 4                      0.29(0.031)
## 5                      0.29(0.027)
## 6                      0.29(0.027)
## 7                      0.29(0.024)
## 8                      0.29(0.022)
## 9                      0.29(0.021)
## 10                     0.29(0.022)
## 11                     0.29(0.023)
## 12                      0.29(0.03)
## 13                     0.29(0.027)
## 14                     0.29(0.026)
## 15                     0.29(0.024)
## 16                      0.3(0.024)
## 17                     0.29(0.028)
## 18                      0.3(0.028)
## 19                     0.29(0.028)
## 20                     0.29(0.027)
## 21                     0.29(0.023)
## 22                     0.29(0.025)
## 23                      0.3(0.025)
## 24                     0.29(0.023)
## 25                     0.29(0.022)
## 26                     0.29(0.029)
## 27                     0.29(0.021)
## 28                     0.29(0.022)
## 29                      0.29(0.02)
## 30                     0.29(0.019)
## 31                     0.29(0.024)
## 32                     0.29(0.025)
## 33                     0.29(0.027)
## 34                     0.29(0.021)
## 35                     0.29(0.024)
## 36                     0.29(0.025)
## 37                     0.29(0.023)
## 38                     0.29(0.019)
## 39                     0.29(0.027)
## 40                     0.29(0.026)
## 41                     0.29(0.023)
## 42                     0.29(0.026)
## 43                     0.29(0.021)
## 44                     0.29(0.028)
## 45                     0.29(0.023)
## 46                     0.29(0.023)
## 47                     0.29(0.029)
## 48                     0.29(0.024)
## 49                     0.29(0.023)
## 50                     0.29(0.023)
## 
## 
##                   Thank&#39;s for using me      
## ------------------------------------------------------
##                                        Hadrien Lorenzo
##                         hadrien.lorenzo.2015@gmail.com
## ======================================================
</code></pre>

<p>The <strong>Cross-Validation results</strong> is a matrix with <strong>4</strong> columns and <strong>N</strong> rows, each row corresponds to a couple \((R,\lambda)\) teste. Each column correpsonds to:</p>

<ul>
<li><strong>R</strong>: The number of components built,</li>
<li><strong>lambda</strong>: The regularization coefficient,</li>
<li><strong>Nb.of.convergences.VS.nb.of.fold</strong>: The number of models, in the cross-validation process, which have converged against the total number of models built.</li>
<li><strong>Mean.sd..time.of.computation</strong>: The mean time of computation for bulding a model and, in parenthesis, its standard deviation.</li>
</ul>

<!-- ## Classification case {.tabset} -->

<!-- ### Build a model {.tabset} -->

<!-- The data set penicilliumYES has 36 rows and 3754 columns, see @clemmensen2007method The variables are 1st order statistics from multi-spectral images of three species of Penicillium fungi: Melanoconidium, Polonicum, and Venetum. These are the data used in the Clemmemsen et al "Sparse Discriminant Analysis" paper. Therefore the structure is, where $\mathbf{Y}$ is the dummy matrix of the $3$ classes : -->

<!-- $$\mathbf{X}\in\mathbb{R}^{36\times3754},\mathbf{Y}\in\mathbb{R}^{36\times3}$$ -->

<!-- ```{r, fig.show='hold',fig.width=7, fig.height=5,message=FALSE,eval=T} -->

<!-- data("penicilliumYES") -->

<!-- X <- penicilliumYES$X -->

<!-- X <- scale(X[,which(apply(X,2,sd)>0)]) &ndash;>

<!-- classes <- c("Melanoconidium","Polonicum","Venetum") -->

<!-- Y <- as.factor(unlist(lapply(classes, -->

<!--                              function(tt){rep(tt,12)}))) -->

<!-- mddsPLS_model_class <- mddsPLS(Xs = X,Y = Y,lambda = 0.956,R = 2, -->

<!--                                mode = "clas",verbose = TRUE) -->

<!-- ``` -->

<!-- #### Plot Method {.tabset} -->

<!-- The **Plot** mehtod permits many types of representation. Among which -->

<!--  * *weights* which shows all the weights under barplot representations. If **super** is set to **TRUE**, then the **super-weights** are represented. In the mono-block case, **super-weights** are equal to **weights**. -->

<!--  * *heatmap* which shows the selected variables (for $\mathbf{X}$ and $\mathbf{Y}$) for the all individuals. -->

<!-- The following sections detailed the discussed method though those parametizations. -->

<!-- ##### *Super-Weight* representation -->

<!-- The following figure permits to represent the *super-weights* for the first and the second components for block 1 (mono-block case actually) and block Y. **mar_left** parameter adds extra values on the left of the plot, usefull to see variable names. -->

<!-- ```{r,fig.height=7,fig.width=10,dpi=DPI,out.width= out.width,out.height=out.height} -->

<!-- plot(mddsPLS_model_class,vizu = "weights",super=T,comp = c(1,2),addY = T,mar_bottom = 3,mar_left = 3,legend.cex = 1,reorder_Y = T) -->

<!-- ``` -->

<!-- ##### *Heatmap* representation {.tabset} -->

<!-- Heatmap permit to appreciate the discriminative aspect of variables according to their individual representations. It has been decided to plot heatmaps component per component, only the first component in the case of that example. -->

<!-- ```{r,fig.height=7,fig.width=10,dpi=DPI,out.width= out.width,out.height=out.height} -->

<!-- plot(mddsPLS_model_class,vizu = "heatmap",comp = 1) -->

<!-- ``` -->

<!-- Each variable has been normalized separately. Dendrogram are used in the individual and in the variable directions using **hclust** similarities. -->

<!-- #### Summary Method -->

<!-- A *summary* method explains more precisely the model. -->

<!-- ```{r, fig.show='hold',message=FALSE,eval=T} -->

<!-- summary(mddsPLS_model_class) -->

<!-- ``` -->

<!-- #### Get selected variables -->

<!-- Output *var_selected* permits to get the selected varaibles per block. -->

<!-- This is a list filled with \code{2R}-column matrices corresponding to the non nul coefficients of each block on the \emph{Super-Components}. They are ordered according to their absolute value on the first **Super-Component**, in a decreasing way. Only $2$ significance digits are kept here for convenience. -->

<!-- ```{r} -->

<!-- signif(mddsPLS_model_class$var_selected[[1]],2) -->

<!-- ``` -->

<!-- ### Plot the two first axes  -->

<!-- ```{r, fig.show='hold',fig.width=10, fig.height=5,message=FALSE,eval=T,dpi=DPI,out.width= out.width,out.height=out.height} -->

<!-- plot(mddsPLS_model_class$mod$T_super,col=Y,pch=as.numeric(Y)+15,cex=2, -->

<!--      xlab="1st X component, 2 var. selected", -->

<!--      ylab="2nd X component, 2 var. selected") -->

<!-- legend(-2,0,legend=classes,col=1:3,pch=15+(1:3),box.lty=0,y.intersp=2) -->

<!-- ``` -->

<!-- ### Cross-validation -->

<!-- The cross-validation process is started in a fold-fixed design, because each sample is repeated $3$ times. In that sense this is a leave-one-out process. $R=2$ fixes the number of dimensions to 2. **NCORES** fixes the number of cores in the paralellized process, **n\_lambda** fixes the number of regularization terms to be tested. -->

<!-- ```{r,fig.width=7, fig.height=6,message=FALSE,eval=F} -->

<!-- n_lambda <- 50 -->

<!-- NCORES <- 7 -->

<!-- res_cv_class <- perf_mddsPLS(X,Y,R = 2,lambda_min=0.95,n_lambda=n_lambda, -->

<!--                              mode = "clas",NCORES = NCORES, -->

<!--                              fold_fixed = rep(1:12,3)) -->

<!-- ``` -->

<!-- The results can be plotted thanks to the next figure which uses the **plot** method. -->

<!-- ```{r,echo=F} -->

<!-- # save(res_cv_class,file="res_cv_class_noXs.RData") -->

<!-- load(file="res_cv_class_noXs.RData") -->

<!-- res_cv_class$Xs <- list(X) -->

<!-- ``` -->

<!-- ```{r,fig.width=10, fig.height=6,message=FALSE,eval=T,dpi=DPI,out.width= out.width,out.height=out.height} -->

<!-- plot(res_cv_class,legend_names = levels(Y),pos_legend="bottomleft") -->

<!-- ``` -->

<!-- Both of the vertical lines set the position of the sparsest models giving the maximum accuracy glogally, right one, and in mean, left one. -->

<!-- The best suited model is chosen for $\lambda\approx0.957$. As the top ruller explains, that model selects $4$ different variables.  -->

<!-- In the following we have performed another cross-validation but the $\lambda$ paramter is not given as an input of the method. In fact we have decided to give a parameter $L_0$ corresponding to the maximum number of variables to be selected in the final model. -->

<!-- ```{r,eval=F} -->

<!-- NCORES <- 7 -->

<!-- res_cv_class_L0 <- perf_mddsPLS(X,Y,R = 2,L0s = 1:4, -->

<!--                              mode = "clas",NCORES = NCORES, -->

<!--                              fold_fixed = rep(1:12,3)) -->

<!-- ``` -->

<!-- ```{r,eval=T,echo=F} -->

<!-- # save(res_cv_class_L0,file="res_cv_class_L0_noXs.RData") -->

<!-- load(file="res_cv_class_L0_noXs.RData") -->

<!-- res_cv_class_L0$Xs <- list(X) -->

<!-- ``` -->

<!-- Which can simply plotted with the following command -->

<!-- ```{r,eval=T,fig.height=6,fig.width=10,dpi=DPI,out.width= out.width,out.height=out.height} -->

<!-- plot(res_cv_class_L0,legend_names = levels(Y), -->

<!--      pos_legend="bottomright",plot_mean = T) -->

<!-- ``` -->

<!-- This synthaxe is particularly well suited when the number of samples is low and so the $(X_j,Y_j)$ are sensible to down-sampling. -->

<!-- It is now clear that the best model is reached for $L_0=3$. -->

<!-- ### Optimal Models {.tabset} -->

<!-- In [@clemmensen2011sparse], only $2$ variableds are selected.  -->

<!-- #### Using $\lambda$ paradigm -->

<!-- ```{r,eval=T,dpi=DPI,out.width= out.width,out.height=out.height} -->

<!-- model_lambda <- mddsPLS(X,Y,R = 2,lambda = 0.957,mode = "cla" ) -->

<!-- plot(model_lambda,super = T) -->

<!-- ``` -->

<!-- This model selects $4$ variables. -->

<!-- #### Using $L_0$ paradigm -->

<!-- ```{r,eval=T,fig.width=10,fig.height=6,dpi=DPI,out.width= out.width,out.height=out.height} -->

<!-- model_L0 <- mddsPLS(X,Y,R = 2,L0 = 3,mode = "cla") -->

<!-- plot(model_L0,super = T,addY = T,pos_legend = "topright",reorder_Y = T,legend.cex = 1) -->

<!-- ``` -->

<!-- This model selects $3$ variables. -->

<h1>Multi-block</h1>

<p>Two different analyses are detailed here, the first one presents the tool and the second one permits to simulate a data-set and details its analysis thanks to the <strong>ddsPLS</strong> tool.</p>

<h2>Toy example {.tabset}</h2>

<p>In the case of missing values it is possible to visualize the missing positions. Let us just consider a 3-blocks toy-example case, based on the previous dataset, such as</p>

<pre><code class="r">data(&quot;liverToxicity&quot;)
X &lt;- scale(liverToxicity$gene)
Y &lt;- scale(liverToxicity$clinic)
p1=p2 &lt;- 1000
p3 &lt;- ncol(X)-p1-p2
Xs &lt;- list(Block1=X[,1:p1],Matrix2=X[,p1+1:p2],Other_Variables=X[,p1+p2+1:p3])
Xs$Block1[1:10,] &lt;- NA
Xs$Matrix2[5+1:10,] &lt;- NA
Xs$Other_Variables[10+1:20,] &lt;- NA

model_multi_vizu &lt;- mddsPLS(Xs,Y,lambda = 0.8,R = 3)
</code></pre>

<h3>Plot Method {.tabset}</h3>

<p>The <strong>Plot</strong> mehtod permits many types of representation. Among which</p>

<ul>
<li><em>weights</em> which shows all the weights under barplot representations. If <strong>super</strong> is set to <strong>TRUE</strong>, then the <strong>super-weights</strong> are represented. In the mono-block case, <strong>super-weights</strong> are equal to <strong>weights</strong>.</li>
<li><em>heatmap</em> which shows the selected variables (for \(\mathbf{X}\) and \(\mathbf{Y}\)) for the all individuals.</li>
</ul>

<p>The following sections detailed the discussed method though those parametizations.</p>

<h4><em>Scaled Super-Weight</em> representation {.tabset}</h4>

<p>The following figure permits to represent the <em>scaled super-weights</em> for blocks 1, 2 and block Y. <strong>mar_left</strong> parameter adds extra values on the left of the plot, usefull to see variable names. Only the first cuper-component is plotted.</p>

<pre><code class="r">plot(model_multi_vizu,vizu = &quot;weights&quot;,super = T,comp=1 ,addY = T,
     mar_left = 5,mar_bottom = 3,reorder_Y = T)
</code></pre>

<p><img src="data:image/png;base64,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" title="plot of chunk unnamed-chunk-17" alt="plot of chunk unnamed-chunk-17" width="1000px" height="1000px" /></p>

<!-- ##### Super-Component 2 -->

<!-- ```{r,fig.height=8,fig.width=13} -->

<!-- plot(model_multi_vizu,vizu = "weights",super = T,comp=2 ,addY = T,mar_left = 5, -->

<!--      mar_bottom = 3,reorder_Y = T) -->

<!-- ``` -->

<!-- ##### Super-Component 3 -->

<!-- ```{r,fig.height=8,fig.width=13} -->

<!-- plot(model_multi_vizu,vizu = "weights",super = T,comp=3 ,addY = T,mar_left = 5,mar_bottom=3,reorder_Y = T) -->

<!-- ``` -->

<h4><em>Heatmap</em> representation {.tabset}</h4>

<p>Heatmap permit to appreciate the discriminative aspect of variables according to their individual representations. It has been decided to plot heatmaps component per component, only the first component in the case of that example.</p>

<p>Each variable has been normalized separately. Dendrogram are used in the individual and in the variable directions using <strong>hclust</strong> similarities.</p>

<pre><code class="r">plot(model_multi_vizu,vizu = &quot;heatmap&quot;,comp = 1)
</code></pre>

<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAANIAAADSCAMAAAAIR25wAAABSlBMVEUAAAAAADoAAGYAOjoAOmYAOpAAZmYAZpAAZrYbnnc6ADo6AGY6OmY6OpA6ZmY6ZpA6ZrY6kJA6kLY6kNtmAABmADpmAGZmOgBmOmZmOpBmZgBmZjpmZmZmZpBmZrZmkGZmkJBmkLZmkNtmtmZmtrZmtttmtv91cLN6UgN9ACWQOgCQOjqQOmaQZgCQZjqQZmaQZpCQkDqQkGaQkJCQkLaQkNuQtpCQtraQttuQtv+Q27aQ29uQ2/+2ZgC2Zjq2Zma2kDq2kGa2kJC2tma2tpC2tra2ttu2tv+225C229u22/+2/7a2/9u2///DIgDZXwLbkDrbkGbbkJDbtmbbtrbb25Db29vb2//b/7bb/9vb///hPADnKYrtYgDyhAD1oQD3ujz40HT75Jr/tmb/tpD/trb/25D/27b/29v/9Lf//7b//8j//9v///9cL3gHAAAACXBIWXMAAASdAAAEnQF8NGuhAAAOqklEQVR4nO2d/X/TuBnAVaAH7AYHDVDggN0xGnaG8s7tRpNCfTRsg0I76AYtbZz0Vq6l/v9/nV4fP5IlW05KkLM88GlkS3afb6zn8aNHskvSsRMyxLGrzf1LG7Jk1nifIk33z9Efu9dXZ26/PNd8+uDJtcLW5aceComQ6fnTp+ZvXJydnj97jpUuzR+/1aJKHrn/zflWe6qVrp69wIu8mtat0o1V+tk+ferZ8VsL0fSG0HKx1SYnaGn1yIMWPfXiwu3L9GRTrMyanj7TZgcz9jNfFKm5f3GWTH1PjtxvUh1e3b5Pjnzf2mnQmrnbKzuNxRZT9YAXWTWtm2EbVPG5iB7XpIVmqpDYP1rav3rv7kq6c/rMKjsZrT2gTZ+cp5XswBEgXXpGLxRFmr7DdL5Mjlwm7JulutJPhkQVu8+KonqqxfYzklUy/Yx9qKu01toh0xu0tLbAkPbphWVIvKvRX3HjwmV54O71L4lkE4YxSJ3UlJRLwUmEHDbSkFKuzmG0GKlMkAZsMVKZIA3YYqQyQRqwxUhlgjRgi5HKBGnAFiOVCdKALUYqE6QBW4xU6o9UPuDLH1J+0i+hqbeU/vYJkmeLLykTJLeFVTrpF5WqSF6HBYtk83t1R7KU2Wd53ytBOnmSf7RJMxYJffbxbqNUV0/xUQwxkGz/0EjxRvxw/duV3W9X4rci6Xs44oWUfRwKkpQ2OfKnq++v3vvwn/aJoa6Sf7BQiFTY97yRfpgnDdb/3jaHQvKutiBxCgL7hkM6NPFEwlcBIaVfG6k8Js31H81Lu5EyZ2H9vcVqvXnDPxZbB/cyj7d7yqvneX5bxUgArSGxf05j8kN6dv3T1bdTC9E315Z+3P1bvI6nrQa4GJZDdfXBalSB79GRnLr7fZXxw+WHP0eN+C514tSFx/gq+ZzYgUX0Ejg1oikP/+HiHArS28bSLGksfrexdPVtkzo+TVHrRSCWjWKk7NIo5W1IWcHV9QZ1DyYJyVdZOpS1ESBhDqw+IXYkh/YDIWW65O4eGilBRoF/mcmrk+SQ0kNFunmTfxxEJ1KwIK7ELtcppofHSEGJYHz3oLPtooGOI0ZabO3MREf/0KJB3q9cw5WYTJFfKdJUi5Gtk5VVsqHbNfgtTRcJRBD+iJGEHDyi/+O7Lx9FpPGUn+2vj0lEnlKkqEHekyWqWJshqb5vMQFkGDYLGTVSusNWY3z3/nGbNGJ+tlsLvNtRlgYvEErYHEypr4SkHXGOTJcoUDek4KUE6eNH9nP/WvrbX94wSdNer6+k14MNXuhxSQyBdqJSa9NDwra7VBJVkLK1tb3NC9tUtpiwgtjcHgxJeOhrb8YFScnTlVCRxDKxdpPeO6fvtNYWopkSJBUIxBsKCQH1DSlCkhUaAPDJTYYjkYCOqQ4FhCQ30oNOvHHwvBM1aCn99A9aKkWC4CdQJLaO9AT8LO14WqgbLJLlMhRdotSJJOxDYmnGkkcCKHGIiQR7hMHQH9i0ukx1aVTSgITwjUGQiKPj1Rcpk1CRuMdbYO4hmp5vMt/nRjLGjBkSUl/35potKSvpQRP2I08sWtIWypYAJ0lA/UTxghXJgvR4L5gT7xxEM89fvPtQgOTqeEEhVfJ4xOUeAkMqthc/JHDhYBoSU9lGkhV0E8qZWw+5dmU8fKObEwa4tYWcOC9WRsqG3RaPFwgSdg9L9HOtVYQkufSrpHUy6FByn+7DcuKqloBCd0QnBcV4m5sGkuYeWDj0vFN3pIruIbXFeMEh2RUvQIKbkzG4qKp1+SEKS24I99dVt1V5s93a2tzcFJCDImUNxgkp1/HCQtIDovZc1ChFUmJF0os9pBp21Nqgt4dbGZvKopQlJYpMGhA4cXtAlL77EM0UDQHrgVTd40HXCxep7FrkamBki514RbGZkYEk98khhnLiAAmh3XCjWqJ+jiNS7lYbEhJKeh2bpc4hKhoCptowcAgkXxGQifoAJDnKYGSbTFD8oCe92oVOXE3y5YaAgSFV8HgkVUhhXyWnxVREcoYCJWGr/RA20DIFUnrg0RUFRVIRxYBIxZF4PZGUhIqkpfkXqMebK0QiDiQQLUOHpyaMVvmvQAOQw3xkSwIhEWEs5FVE4LAlzAqPaiHNXxTjZUhFHu/rI1X1eHi1RrBIFtWLkUhB7sE9BKogggJB9kR+CJDAlgSHtCUZSAyEZOt4tUcK+yqhUe2xWRYQOWO8QiQp/kg+LRGSZkts3zYyn4I8HvN3xR6PqFVYDo8XAlJFjwdrmB0db0ABzwZFDQl6G0divRB1vWFtaVyRKsV4XwEpl/QqsSW0w5i5sEHZpzbVPIdj3jMHxUezgkQiQR5ZSyBzsbgH9xCwJkje7kEtEnTPqIeD5GlLKGowr5IG4DeYQPjasFUHUlBdEdbBJAYMbpHvFkLNqhqSYylH/ZHc6ZRAkDSPNxc1XOsecmNa64x6ZVHZZPOrkCGDAOiJka703VnAl6VTlCGZAVFhmr8+SP4eD/JDRUs5wkCqZksFuQdt1gEV89YFiwn6ch2B2UabMgM6aUvZNIYaWogps8qjWjWksFyluiMhCRVJGwIWjJcsSDe5pKl204eZVc0jg4/m/rcvlw8kuTVEqGXPWGIkvgteYNErHzt1u5ubr19viTVscviUi/EabUceDzsG2fECRari8cyKYJEsynsilc1clKwC4Eh7e/ZK+c0AncCSCeSumreFkGFrCxZQVkdye7z6IoV/lXyXchCVDs92KVty6e0j9ihXuwlwOhURw4QFhxJAMmRl9kX3VYrxsgy/7vECQ6rq8XBlsEgW5R17iR7iDThXq1uYe3gF3hvdFpj/3lbPweCFoGruoqJ7SNWjacPNqIeHNMi6B4jn/IYhuTweKm6LR0jMGE94PL7G3xLjOT2eCvNCR/JO8yOkyh1vxFfJ3+OZj1ykH7kIpMQvjeIpCEqk8NCmHEYkaGW/SufxXQ7lXXvHEUkbXASKdBAdj06sNg+Wl29daa0tdGgMYUXKBXjGVfqCkkvrZSIjPIbDUipyKcfvH6Jjv8Tr0cwyW8vf2XVMbNYJybPjGUi5jldvJMj1jwRJDWh7MDxGYYVAev36tYopBkVShXFBItnrQ0JFWp3rsNeg7Hb++6rjhVQSEPk5cyNAQHsSYMgo0IEygQxIcq5Wha7ciXeWZpu/b3z6d3t6PWp+Ggekih0vW8jhivHqhqTXWke1WaSJ1Yd9TmJULQ5A50ORrmRBeRWwJQk2AFJJ7qGGSEhCRdKGgA1WGg7Jra5ZdLYB1dFDrSKVsr0NLWUmj9kSRVIrpNhcrflAd8cDqXLSa7RIvkNADSnsq2TTvggJL+aokCEC6yrw8Tkj7CGzYlXMRcM+OWKXQ/WhkFDHGwukgVOTI0XyXLymAqKSBHJBCsjISIKiOYRsLRX7IVeLGwfJpVIMS87VQo6yLM2PUw2pMbsZKlKJx0PD2DR3cwoWqdCWqiL5Bq15q8k16GmPMQgKvrfbFe/rkKvx5OoUMVD3mLnIIZV0vCCQ9Hd6tZv6KyxMJFLiHvygACjX2Mwd2w4VBTlZwW+04lYrE3vGwz7/fPfh+YsCJI8ps6+PVM09eExshoDka0tGr0uHfuaiL5ajaaDmplguKZ2fXCcu1nBIkfdXWLpWxeOZNeOFVBDjBYFkPN5IB4HFSI5IHD2/ZL4cIW8RyGJgyUZOEkPQE5tIwImbr7AAj9ee01/aY0HKdgaKVOzxiB3J0vGCQiqyJRNJ93rZcl2ktd+jVigv4vL7iZFVgfcxKsOSSNrgotw9GEhG7dggZbfaUJHM1fyax7MiWa5SNv9vANkBlVr2AAGa6ANhBQUv6MDLBNRs7bbtpT2ax7MgFb0+MwykSh7PkGCRKtoSEjVlJmMAoT4bzfTVWhS5LJePTJFrR8tUTFetBKXv5Iq1vC0xDjlxJizJ5yWnxOIV8use6oBkJFFcEijSQXS889sfl1/dutJqN9LFGx0ts+WHZLeGEgHdYNveRuSJE7E4XBqRdNc//WQ7MXfdx36+8Pfl553lzi57DdEnYrsJFXW8wJCQLLag41nMqj5XySKVkZArhqJ45iXJ9ibK2KQJdaWuyluD8kJ4BMpCCzGskuPwbnePCdvH7WhvDw4SqF1X0qsEyex444AU+FWSSzmUu8shWR2DiaQ5157w23mknqoBJDQ6VxEBIHGnLNy95GNgEohjJsnnz7BiQDF3+VKOuWh6Xbm7PJJPxwsKSYlydwZS4c0WdzwkfSTOCsvroFCPE94tUS23hb6yUtTAOWU1iN1UrNmHsUIqu9UGhnQQHbv4oNW+cekDhEQGkrvv5TyeOCfcOfvaCgz1IAja22OGIQ0IsiXoEfR+H6aapKEoe5OjDJFDSZhRqSCQ2lJ76kY0F8082oWQKPdsZt2QsAgfkUfy6HihItlsqUCwE5duGHo00wR56UQNY+XDvtJiWAENMdDxMGSVWWNxBnRb5Wa2t/f5M7/5im+LF/+fkYo7XlBI1OPdaa4tdNZai08uLS3cfrDQsSNZ2FU6pWsRLc+bGAIpEVA+awVHy+9FfheqQR8lDft9BiShVFTRZx7v7HI0s0y93tzDaI4W649k7XjuUNWWx6sDknOnDneSC4StSBdkGKAuQtVIEJDxTUBuGX0LXHU5RmEcn6lIs5Kjl6pI2lUaG6RMQkUyJjZdD3SbFmUimVaTUzdBOiPloWX2leB9QrgBaSuLZEjLgPaUiGCkX+W9rakRloeKVO3Ps+mj3GCRHGZjR9L6XobERRu1IEW1vVJnlMdzIcmTsLP2VBZaunbAFLa0J4yqqnvQqHIdr95ISEJFyr2puuyvNwYveY9X8AdW6iKV/iBlTWWCVAeZINVBJkh1kAlSHWSCVAeZINVBRoj0rxX6Y/9xvJGm8cb+Y7GTbR2yjBLpz+dbO0evL/2y/yJ+e/R6/P7xzlQrfjZ92FCjRLry8tGj3etxvLyy9OMuQ3oeNeL5M3VG+uH94zZpxOtn0nieNNpkJiKNpVnSPOTfM4bu4X+4slYEjgHl5wAAAABJRU5ErkJggg==" title="plot of chunk unnamed-chunk-18" alt="plot of chunk unnamed-chunk-18" width="1000px" height="1000px" /></p>

<h3>Summary Method</h3>

<p>A <em>summary</em> method explains more precisely the model. </p>

<pre><code class="r">summary(model_multi_vizu)
</code></pre>

<pre><code>## ===============================================================
##                      ddsPLS object description              
## ===============================================================
## 
## Number of blocks: 3
## Number of dimensions: 3
## Regularization coefficient: 0.8
## Number of individuals: 64
## Number of variables in Y part: 10
## Model built in mode regression
## Maximum number of iterations in the imputation process: 50
## Algorithm of imputation has converged
## 
## 
##           Variance Explained (%)    
## -------------------------------------------
## Total Y variance explained by all the Super Components
## [1] 71
## Total Y variance explained by each Super Component
## [1] 61 55 47
## 
## Marginal Y variable variance explained by each Super Component
##                    Super Comp. 1 Super Comp. 2 Super Comp. 3
## BUN.mg.dL.                  72.0            70          44.0
## Creat.mg.dL.                17.0            23           8.7
## TP.g.dL.                     6.8            20          30.0
## ALB.g.dL.                   20.0            32          26.0
## ALT.IU.L.                   95.0            82          65.0
## SDH.IU.L.                   16.0            15          39.0
## AST.IU.L.                   94.0            78          64.0
## ALP.IU.L.                   41.0            42          32.0
## TBA.umol.L.                 85.0            70          84.0
## Cholesterol.mg.dL.          66.0            60          28.0
## 
## Total Y variance explained by each component of each block
##                 Comp. 1 Comp. 2 Comp. 3
## Block1               56      46      43
## Matrix2              60      49      56
## Other_Variables      60      31      51
## 
## 
##                     RV coefficient    
## -------------------------------------------------------
## Total Y with all the Super Components
## [1] 0.3
## 
## Total Y with each Super Component
## [1] 0.37 0.30 0.22
## 
## Each Y variable with each Super Component
##                    Super Comp. 1 Super Comp. 2 Super Comp. 3
## BUN.mg.dL.                0.5100         0.490        0.1900
## Creat.mg.dL.              0.0300         0.051        0.0076
## TP.g.dL.                  0.0047         0.039        0.0890
## ALB.g.dL.                 0.0390         0.100        0.0690
## ALT.IU.L.                 0.9100         0.680        0.4200
## SDH.IU.L.                 0.0250         0.023        0.1500
## AST.IU.L.                 0.8900         0.610        0.4000
## ALP.IU.L.                 0.1700         0.180        0.1000
## TBA.umol.L.               0.7200         0.490        0.7100
## Cholesterol.mg.dL.        0.4300         0.360        0.0770
## 
## Total Y with each component of each block
##                 Comp. 1 Comp. 2 Comp. 3
## Block1             0.32   0.210    0.19
## Matrix2            0.37   0.240    0.31
## Other_Variables    0.37   0.095    0.26
## 
## 
##          Missing value information    
## -------------------------------------------
## 
##                                   Block1 Matrix2 Other_Variables  Total
## Number of variables               1000.0  1000.0          1116.0 3116.0
## Number of missing samples           10.0    10.0            20.0   40.0
## Proportion of missing samples (%)   15.6    15.6            31.2   20.8
## 
## 
##               mddsPLS results         
## -------------------------------------------
## 
## At most 39 variable(s) can be selected in the X part
## 
## 
##  ---- For each block of X, are selected
##                 Super Comp. 1 Super Comp. 2 Super Comp. 3
## Block1                      3             3             3
## Matrix2                    29            29            29
## Other_Variables             7             7             7
##  ---- For the Y block, are selected
##         @ (4,4,4) variable(s)
## 
## 
##                  Thank&#39;s for using me      
## ---------------------------------------------------------------
##                                                 Hadrien Lorenzo
##                                  hadrien.lorenzo.2015@gmail.com
## ===============================================================
</code></pre>

<h3>Get selected variables</h3>

<p>Output <em>var_selected</em> permits to get the selected varaibles per block.</p>

<p>This is a list filled with \code{2R}-column matrices corresponding to the non nul coefficients of each block on the \emph{Super-Components}. They are ordered according to their absolute value on the first <strong>Super-Component</strong>, in a decreasing way. Only \(2\) significance digits are kept here for convenience.</p>

<p>Here is represented its value for the first block</p>

<pre><code class="r">model_multi_vizu$var_selected[[1]]
</code></pre>

<pre><code>##              Weights_comp_1 Weights_comp_2 Weights_comp_3
## A_43_P16842       0.8112084     -0.5844501     0.01894704
## A_43_P18824       0.2021202      0.3106499     0.92878632
## A_42_P470649      0.5487152      0.7496097    -0.37013102
##              Super_Weights_comp_1 Super_Weights_comp_2
## A_43_P16842           -0.05547870          -0.03013106
## A_43_P18824           -0.02591159          -0.21819422
## A_42_P470649          -0.07391688           0.02654694
##              Super_Weights_comp_3
## A_43_P16842           -0.25492936
## A_43_P18824            0.01562498
## A_42_P470649           0.03112857
</code></pre>

<!-- ## Simulation Model {.tabset} -->

<!-- In the following is considerd a simulated dataset to have an idea of the behavior of the method in the multi-block context with missing values. This part is based on the theoretical aspects of the methodology. -->

<!-- ### Notations -->

<!-- According to what have been proposed in [@johnstone2004sparse], the following simulations follow the spike covariance models -->

<!-- $$\left\{\begin{aligned} -->

<!-- {\bf X}_1&= {\bf L}{\bf \Omega}_1^{1/2}{\bf U}^T_{1,mod}+{\bf E}_1\\ -->

<!-- &\vdots\\ -->

<!-- {\bf X}_T&= {\bf L}{\bf \Omega}_T^{1/2}{\bf U}^T_{T,mod}+{\bf E}_T\\ -->

<!-- {\bf Y}&= {\bf L}{\bf \Omega}_y^{1/2}{\bf V}^T_{mod}+{\bf E}_y\\ -->

<!-- \end{aligned} -->

<!-- \right.,$$ -->

<!-- where $({\bf \Omega}_t)_{t=1\cdots T}$ and ${\bf \Omega}_y$ are $R$-dimensional diagonal matrices with strictly positive diagonal elements. $({\bf U}_{t,mod}\in\mathbb{R}^{p_t\times R})_{t=1\cdots T}$ and ${\bf V}_{mod}\in\mathbb{R}^{q\times R}$ are  matrices with orthonormal columns. ${\bf L}\in\mathbb{R}^{n\times R}$ is a matrix where elements are i.i.d. standard Gaussian random effects, $({\bf E}_t\in\mathbb{R}^{n\times p_t})_{t=1\cdots T}$ (respectively ${\bf E}_y\in\mathbb{R}^{n\times q}$) are matrices such that each row follows the standard multivariate normal distribution $(N_{p_t}(0,\mathbb{I}_{p_t}))_{t=1\cdots T}$ (respectively $N_q(0,\mathbb{I}_q)$) and the $n$ rows are independent and mutually independent noise vectors. Let us mention that the matrix ${\bf L}$ does not depend of $t$ and thus introduces a common structure between the ${\bf X}_t$'s and $\bf Y$ models. -->

<!-- ### Fix the Parameters -->

<!-- Usual parameters used to simulate datasets -->

<!-- ```{r,fig.width=7, fig.height=6,message=FALSE,eval=F} -->

<!-- n <- 50 # number of individuals -->

<!-- R <- 5 # number of created dimensions in __L__ -->

<!-- T_ <- 5 # number of blocks -->

<!-- sd_error <- 0.1 # Standard-deviation of the spike-covariance model element matrices of $E_t$ and $E_y$ -->

<!-- p_s <- sample(x = 100:200,size = T_,replace = T) # number of variables per block $X_t$ -->

<!-- q <- 10  # number of variable in $Y$ -->

<!-- R_real <- 3 # number of components of __L__ described in __Y__ -->

<!-- p_missing <- 0.3 # the proportion of missing values -->

<!-- ``` -->

<!-- Possible values for $({\bf \Omega}_t^{1/2})_{t=1\cdots T}$ and ${\bf \Omega}_y^{1/2}$ diagonal elements are then chosen. It has been chosen to consider low elements, close to $0$, and high elements, $\approx 1$. -->

<!-- ```{r,fig.width=7, fig.height=6,message=FALSE,eval=F} -->

<!-- o_x <- seq(0,1,length.out = 1000) -->

<!-- o_y <- (o_x-0.5)^2 -->

<!-- o_y[which(o_y<0.2)] <- 0 # keep only low or high potential diagonal elements -->

<!-- all_omegas <- sample(o_x,prob = o_y,size = R*T_) # Select R*T_ elements -->

<!-- all_omegas_y <- sample(o_x,prob = o_y,size = R_real) # Select R_real elements -->

<!-- Omegas_y <- diag(c(all_omegas_y,rep(0,R-R_real))) # Create the Omega_y diagonal matrix -->

<!-- ``` -->

<!-- ### Generate Covariate Dataset -->

<!-- __Xs__ is a list of matrices corresponding to the defined spike-covariance model. -->

<!-- ```{r,message=FALSE,eval=F} -->

<!-- Xs <- list() -->

<!-- L <- matrix(rnorm(n*R),nrow = n) -->

<!-- for(k in 1:T_){ -->

<!--     Omegas <- diag(all_omegas[1:R+(k-1)*R]) -->

<!--     Us <- svd(matrix(rnorm(p_s[k]*n),nrow = n))$v[,1:R] -->

<!--     E_k <- matrix(rnorm(n*p_s[k],sd = sd_error),nrow = n) -->

<!--     Xs[[k]]<- scale(E_k + tcrossprod(L%*%Omegas,Us)) -->

<!-- } -->

<!-- ``` -->

<!-- A proportion $p_{missing}$ of the data is missing, the following script permits to remove that proportion of samplestaking into account that a given participant must not be missing for all blocks. -->

<!-- ```{r,message=FALSE,eval=F} -->

<!-- values <- expand.grid(1:n,1:T_) -->

<!-- values_id <- 1:(n*T_) -->

<!-- probas <- rep(1,n*T_)/(n*T_) -->

<!-- number_miss_samp <- floor(n*T_*p_missing) -->

<!-- missin_samp <- matrix(NA,nrow = number_miss_samp,ncol = 2) -->

<!-- for(sam in 1:number_miss_samp){ -->

<!--   curr_id <- values_id[sample(values_id,size = 1,prob = probas)] -->

<!--   missin_samp[sam,1] <- values[curr_id,1] -->

<!--   missin_samp[sam,2] <- values[curr_id,2] -->

<!--   probas[curr_id] <- 0 -->

<!--   if(length(which(na.omit(missin_samp[,1])==missin_samp[sam,1]))==n){ -->

<!--     probas[which(values[,1]==missin_samp[sam,1])] <- 0 -->

<!--   } -->

<!--   Xs[[missin_samp[sam,2]]][missin_samp[sam,1],] <- NA ## Remove individual value -->

<!-- } -->

<!-- ``` -->

<!-- ### Generate Response Matrix -->

<!-- __Y__ is a matrix also corresponding to the defined spike-covariance model. -->

<!-- ```{r,message=FALSE,eval=F} -->

<!-- V <- svd(matrix(rnorm(q*n),nrow = n))$v[,1:R] -->

<!-- E_y <- matrix(rnorm(q*n,sd = sd_error),nrow = n) -->

<!-- Y <- tcrossprod(L%*%Omegas_y,V) -->

<!-- Y <- scale(E_y + Y) -->

<!-- ``` -->

<!-- ### Build a model {.tabset} -->

<!-- ```{r,echo=F} -->

<!-- # save(Xs,Y,file="Xs_y_multi.RData") -->

<!-- load("Xs_y_multi.RData") -->

<!-- ``` -->

<!-- A model is simply built as follows -->

<!-- ```{r} -->

<!-- model <- mddsPLS(Xs,Y,lambda = 0.75,R = 3,verbose = T) -->

<!-- ``` -->

<!-- #### Plot Method {.tabset} -->

<!-- The **Plot** mehtod permits many types of representation. Among which -->

<!--  * *weights* which shows all the weights under barplot representations. If **super** is set to **TRUE**, then the **super-weights** are represented. In the mono-block case, **super-weights** are equal to **weights**. -->

<!--  * *heatmap* which shows the selected variables (for $\mathbf{X}$ and $\mathbf{Y}$) for the all individuals. -->

<!-- The following sections detailed the discussed method though those parametizations. -->

<!-- ##### *Scaled Super-Weight* representation {.tabset} -->

<!-- The following figure permits to represent the *scaled super-weights* for blocks **X** and block **Y**. **mar_left** parameter adds extra values on the left of the plot, usefull to see variable names. -->

<!-- ###### Super-Component 1 -->

<!-- ```{r,fig.height=10,fig.width=10} -->

<!-- plot(model,vizu = "weights",super = T, -->

<!--      comp=1 ,addY = T,mar_left = 0) -->

<!-- ``` -->

<!-- ###### Super-Component 2 -->

<!-- ```{r,fig.height=10,fig.width=10} -->

<!-- plot(model,vizu = "weights",super = T, -->

<!--      comp=2 ,addY = T,mar_left = 0) -->

<!-- ``` -->

<!-- ###### Super-Component 3 -->

<!-- ```{r,fig.height=10,fig.width=10} -->

<!-- plot(model,vizu = "weights",super = T, -->

<!--      comp=3 ,addY = T,mar_left = 0) -->

<!-- ``` -->

<!-- ##### *Heatmap* representation {.tabset} -->

<!-- Heatmap permit to appreciate the discriminative aspect of variables according to their individual representations. It has been decided to plot heatmaps component per component, first and second only here. -->

<!-- ###### Super-Component 1 -->

<!-- ```{r,fig.height=13} -->

<!-- plot(model,vizu = "heatmap",comp = 1) -->

<!-- ``` -->

<!-- ###### Super-Component 2 -->

<!-- ```{r,fig.height=13} -->

<!-- plot(model,vizu = "heatmap",comp = 2) -->

<!-- ``` -->

<!-- Each variable has been normalized separately. Dendrogram are used in the individual and in the variable directions using **hclust** similarities. -->

<!-- #### Summary Method -->

<!-- A *summary* method explains more precisely the model. -->

<!-- ```{r, fig.show='hold',message=FALSE,eval=T} -->

<!-- summary(model,plot_present_indiv = T) -->

<!-- ``` -->

<!-- The package **eulerr** described in [@eulerrpackage], permits to visualize the missing values through Euler Diagramm such as follows -->

<!-- In each area is detailed the number of individuals present. So **Block1** has $5$ missing samples in common with **Matrix2** and **Other_Variables** has $10$ missing samples which are present in the other matrices. -->

<!-- ## Cross-validation {.tabset} -->

<!-- The cross-validation process is started in a leave-one-out design along $R\in\{1,2,3\}$ dimensions. -->

<!-- ```{r,message=FALSE,eval=F} -->

<!-- n_lambda <- 20 -->

<!-- NCORES <- 7 -->

<!-- cross_valid <- perf_mddsPLS(Xs,Y,lambda_min = 0.2, -->

<!--                             n_lambda = n_lambda, -->

<!--                             R = 1,kfolds = "loo",NCORES = NCORES) -->

<!-- cross_valid_2 <- perf_mddsPLS(Xs,Y,lambda_min = 0.2, -->

<!--                             n_lambda = n_lambda, -->

<!--                             R = 2,kfolds = "loo",NCORES = NCORES) -->

<!-- cross_valid_3 <- perf_mddsPLS(Xs,Y,lambda_min = 0.2, -->

<!--                             n_lambda = n_lambda, -->

<!--                             R = 3,kfolds = "loo",NCORES = NCORES) -->

<!-- ``` -->

<!-- ```{r,echo=F} -->

<!-- # save(cross_valid,cross_valid_2,cross_valid_3,file="cross_valid_multi.RData") -->

<!-- load("cross_valid_multi_noXs.RData") -->

<!-- cross_valid$Xs=cross_valid_3$Xs=cross_valid_3$Xs <- Xs -->

<!-- cross_valid$Y=cross_valid_3$Y=cross_valid_3$Y <- Y -->

<!-- ``` -->

<!-- ### Comparison of Results for Varying *R* {.tabset} -->

<!-- In the following we compare the results for the different values of $\lambda$, classically, but also $R$. -->

<!-- #### One component {.tabset} -->

<!-- ##### Error Plot -->

<!-- ```{r,fig.width=12, fig.height=7} -->

<!-- plot(cross_valid,plot_mean = T,ylim=c(0,1.2),no_occurence = T, -->

<!--      which_sd_plot = c(3,5,6,8,10),alpha.f = 0.2, -->

<!--      legend_names = 1:10,pos_legend="topleft") -->

<!-- ``` -->

<!-- ##### Summary Information -->

<!-- ```{r} -->

<!-- summary(cross_valid,plot_res_cv = F) -->

<!-- ``` -->

<!-- ##### Conclusion for One Component -->

<!-- The error curves admit errors around $\lambda=0.4$ and almost all calculus have converged. -->

<!-- #### Two components {.tabset} -->

<!-- ##### Error Plot -->

<!-- ```{r,fig.width=12, fig.height=7} -->

<!-- plot(cross_valid_2,plot_mean = T,ylim=c(0,1.2),no_occurence = T, -->

<!--      which_sd_plot = c(3,5,6,8,10),alpha.f = 0.2, -->

<!--      legend_names = 1:10,pos_legend="topleft") -->

<!-- ``` -->

<!-- ##### Summary Information -->

<!-- ```{r} -->

<!-- summary(cross_valid_2,plot_res_cv = F) -->

<!-- ``` -->

<!-- ##### Conclusion for Two Components -->

<!-- Errors are smaller for *low valued regularization coefficients* than in the case $R=1$. It means that the model can learn the general structure of the **Y** dataset. The **mean MSEP** is around $0.2$ in that region while it was around $0.5$ for the case $R=1$. -->

<!-- For larger $\lambda$, it is interesting to notice that some variables are well predicted until $\lambda=0.75$. -->

<!-- Looking at the **summary** shows that the algorithm found information almost all the time. -->

<!-- #### Three components {.tabset} -->

<!-- ##### Error Plot -->

<!-- ```{r,fig.width=12, fig.height=7} -->

<!-- plot(cross_valid_3,plot_mean = T,ylim=c(0,1.2),no_occurence = T, -->

<!--      which_sd_plot = c(3,5,6,8,10),alpha.f = 0.2, -->

<!--      legend_names = 1:10,pos_legend="topleft") -->

<!-- ``` -->

<!-- ##### Summary Information -->

<!-- ```{r} -->

<!-- summary(cross_valid_3,plot_res_cv = F) -->

<!-- ``` -->

<!-- ##### Conclusion for Three Components -->

<!-- Most of the calculus for low $\lambda$ have not converged and the errors are not interestingly shrinked. -->

<!-- One would say that there is no information in that third component. -->

<!-- But for large values of $\lambda$, around $0.6$, the algorithm converges quite often and it seems that a group of $6$ variables is very finely predicted, while $2$ others are averagely predicted and the laste$2$ ones are poorly described. -->

<!-- ### Conclusion of the Comparisons -->

<!-- According to the previous part it seems interesting to fix -->

<!--  * $R=2$ and low $\lambda$ permits to describe accurately the structure of the $\mathbf{Y}$ data set. -->

<!--  * $R=3$ and $\lambda\approx0.6$ permits to more precisely predict $6$ variables and not the others. -->

<!-- We will now build the corresponding models and see its specificities. -->

<!-- ## Two Models, a General and a Specific {.tabset} -->

<!-- In the following we have built the two models discussed before -->

<!-- ### A General Model -->

<!-- The one defined for $R=2$ and $\lambda\approx0.2$ which is built as follows -->

<!-- ```{r} -->

<!-- model_general <- mddsPLS(Xs,Y,R=2,lambda=0.2,verbose = T) -->

<!-- ``` -->

<!-- More precise information can be found using the summary method -->

<!-- ```{r} -->

<!-- summary(model_general) -->

<!-- ``` -->

<!-- The Euler diagramm represents the missing samples positions and overlappings among the different data sets. -->

<!-- ### A Specific Model -->

<!-- The one defined for $R=3$ and $\lambda\approx0.6$ which is built as follows -->

<!-- ```{r} -->

<!-- model_specific <- mddsPLS(Xs,Y,R=3,lambda=0.6,verbose = T) -->

<!-- ``` -->

<!-- More precise information can be found using the summary method -->

<!-- ```{r} -->

<!-- summary(model_specific) -->

<!-- ``` -->

<!-- The Euler diagramm represents the missing samples positions and overlappings among the different data sets. -->

<!-- <i class="fa fa-exclamation-triangle fa-3x" aria-hidden="true"></i> **The Euler diagram problem becomes hard when the number of sets is to large and in those contexts can be wrong.** <i class="fa fa-exclamation-triangle fa-1x" aria-hidden="true"></i> &ndash;>

<!-- Here blocks 1 and 2 are not selected and no more than $21$ variables is used per block. -->

<h1>References</h1>

</body>

</html>
